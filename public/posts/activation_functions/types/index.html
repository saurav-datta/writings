<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Types | Thoughts and Notes</title>
<link rel="apple-touch-icon" sizes="57x57" href="/favicon_io/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/favicon_io/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/favicon_io/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/favicon_io/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/favicon_io/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/favicon_io/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/favicon_io/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/favicon_io/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/favicon_io/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="/favicon_io/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon_io/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon_io/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon_io/favicon-16x16.png">
<link rel="manifest" href="/favicon_io/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/favicon_io/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">


    <link rel="stylesheet" href="/css/main.css">


      <script src="/js/main.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>


</head>
<body>
  <header>
    <header>
  <nav class="path-nav">
    <ol>
      
  
    
  
    
  
    
  
  <li>
    /
    <a href="/">Thoughts and Notes</a>
    /
  </li>

  
  <li>
    
    <a href="/posts/">Posts</a>
    /
  </li>

  
  <li>
    
    <a href="/posts/activation_functions/">Activation functions</a>
    /
  </li>

  
  <li class="current">
    
    <a href="/posts/activation_functions/types/">Types</a>
    
  </li>

    </ol>
  </nav>
</header>



  </header>
  <main>
    
  <h1>Types</h1>
  
  <div class="terms-list">
    <ul>
        <li><a href="/tags/activation-functions/">[Activation-Functions]</a></li>
    </ul>
  </div>


  
  
    <nav class="toc">
      <strong>Table of contents</strong>
      <div class="toc-content">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#comparison">Comparison</a></li>
    <li><a href="#sigmoid">Sigmoid</a></li>
    <li><a href="#softmax">Softmax</a></li>
    <li><a href="#hard-sigmoidswish">Hard sigmoid/swish</a></li>
    <li><a href="#tanh">Tanh</a></li>
    <li><a href="#dynamic-tanh">Dynamic Tanh</a></li>
    <li><a href="#relu">ReLU</a></li>
    <li><a href="#leaky-relu">Leaky Relu</a></li>
    <li><a href="#parametric-relu-prelu">Parametric ReLU (PReLU)</a></li>
    <li><a href="#elu">ELU</a></li>
    <li><a href="#swish">Swish</a></li>
    <li><a href="#gelu">GELU</a></li>
  </ul>
</nav>
      </div>
    </nav>
  

  <h1 id="types-of-activation-functions">Types of activation functions</h1>
<h2 id="comparison">Comparison</h2>
<table>
  <thead>
      <tr>
          <th><strong>Activation Function</strong></th>
          <th><strong>Pros</strong></th>
          <th><strong>Cons</strong></th>
          <th><strong>Used In LLMs</strong></th>
          <th><strong>Common Use Cases</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Sigmoid</strong></td>
          <td>Smooth, probabilistic output between (0,1)</td>
          <td>Vanishing gradient, not zero-centered</td>
          <td>None (not used in modern LLMs)</td>
          <td>Binary classification (non-LLM)</td>
      </tr>
      <tr>
          <td><strong>Softmax</strong></td>
          <td>Normalized output probabilities</td>
          <td>Sensitive to large input values</td>
          <td><strong>All LLMs (e.g., GPT, BERT, LLaMA, T5, Claude, Gemini, PaLM)</strong></td>
          <td>Final output/token prediction</td>
      </tr>
      <tr>
          <td><strong>Hard Sigmoid/Swish</strong></td>
          <td>Low compute, approximates sigmoid/swish</td>
          <td>Less precise</td>
          <td><strong>MobileBERT, TinyBERT, DistilBERT (quantized/edge LLMs)</strong></td>
          <td>Edge/NLP deployment</td>
      </tr>
      <tr>
          <td><strong>Tanh</strong></td>
          <td>Zero-centered, smooth gradient</td>
          <td>Vanishing gradients at extremes</td>
          <td>Early LSTM-based LLMs (e.g., early Seq2Seq models)</td>
          <td>Historical LLMs, RNN-based</td>
      </tr>
      <tr>
          <td><strong>Dynamic Tanh</strong></td>
          <td>Adaptive shape, better expressiveness</td>
          <td>More parameters, added complexity</td>
          <td>Experimental adaptive LLMs (research only)</td>
          <td>Meta-learning, adaptive time-series</td>
      </tr>
      <tr>
          <td><strong>ReLU</strong></td>
          <td>Efficient, avoids vanishing gradient for positive inputs</td>
          <td>Dying ReLU problem</td>
          <td>Rare; used in early transformer variants (e.g., Transformer-XL)</td>
          <td>Legacy LLM components</td>
      </tr>
      <tr>
          <td><strong>Leaky ReLU</strong></td>
          <td>Mitigates dying ReLU issue</td>
          <td>Slightly more complex than ReLU</td>
          <td>Rare in LLMs; minor experimentation (e.g., early GShard)</td>
          <td>Not typical in production LLMs</td>
      </tr>
      <tr>
          <td><strong>Parametric ReLU (PReLU)</strong></td>
          <td>Learns slope of negative input</td>
          <td>Risk of overfitting, adds parameters</td>
          <td>Rare, mostly experimental in LLMs</td>
          <td>Not standard in transformer LLMs</td>
      </tr>
      <tr>
          <td><strong>ELU</strong></td>
          <td>Smooth for negative inputs, zero-centered</td>
          <td>Higher compute cost</td>
          <td>Not used in LLMs</td>
          <td>Not applicable</td>
      </tr>
      <tr>
          <td><strong>Swish</strong></td>
          <td>Smooth, non-monotonic, better than ReLU in some cases</td>
          <td>More computation</td>
          <td><strong>T5, Switch Transformer, GShard</strong></td>
          <td>NLP transformer blocks</td>
      </tr>
      <tr>
          <td><strong>GELU</strong></td>
          <td>Smooth, noise-tolerant, better convergence</td>
          <td>Computationally intensive</td>
          <td><strong>GPT-2, GPT-3, GPT-4, BERT, RoBERTa, T5, LLaMA, OPT, BLOOM, Mistral, Falcon</strong></td>
          <td>Feedforward layers in LLMs</td>
      </tr>
  </tbody>
</table>
<h2 id="sigmoid">Sigmoid</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> math<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
</span></span></code></pre></div><h2 id="softmax">Softmax</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(x):
</span></span><span style="display:flex;"><span>    exp_x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(x <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>max(x))
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> exp_x <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sum(exp_x, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> res
</span></span></code></pre></div><h2 id="hard-sigmoidswish">Hard sigmoid/swish</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hard_sigmoid_swish</span>(x):
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>clip((x <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> res
</span></span></code></pre></div><h2 id="tanh">Tanh</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tanh</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x)) <span style="color:#f92672">/</span> (np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
</span></span></code></pre></div><h2 id="dynamic-tanh">Dynamic Tanh</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">DyTanh</span>(x, gamma, alpha , beta):
</span></span><span style="display:flex;"><span>     
</span></span><span style="display:flex;"><span>    alpha_x <span style="color:#f92672">=</span> alpha <span style="color:#f92672">*</span> x
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> gamma <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>tanh(alpha_x) <span style="color:#f92672">+</span> beta
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> res
</span></span><span style="display:flex;"><span><span style="color:#75715e"># alpha is a learnable scalar parameter </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># that allows scaling the input differently based on its range, accounting  for varying x scale</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># gamma, beta are learnable, per-channel vector parameters, </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># the same as those used in all normalization layers</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># they allow the output to scale back to any scales.</span>
</span></span></code></pre></div><ul>
<li>
<p><strong>Reference</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2503.10622">Arxiv-Transformers without Normalization</a></li>
</ul>
</li>
<li>
<p><strong>Pros</strong></p>
<ul>
<li>Zero-centered, smooth gradient</li>
</ul>
</li>
<li>
<p><strong>Cons</strong></p>
<ul>
<li>Vanishing gradients at extremes</li>
</ul>
</li>
</ul>
<h2 id="relu">ReLU</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">relu</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>maximum(<span style="color:#ae81ff">0</span>, x)
</span></span></code></pre></div><h2 id="leaky-relu">Leaky Relu</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">leaky_relu</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, x, <span style="color:#ae81ff">0.01</span> <span style="color:#f92672">*</span> x)
</span></span></code></pre></div><h2 id="parametric-relu-prelu">Parametric ReLU (PReLU)</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">param_relu</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, x, a <span style="color:#f92672">*</span> x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># a is learnable</span>
</span></span></code></pre></div><h2 id="elu">ELU</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">elu</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, x, alpha <span style="color:#f92672">*</span> (np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
</span></span></code></pre></div><h2 id="swish">Swish</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">elu</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (x <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x)))
</span></span></code></pre></div><h2 id="gelu">GELU</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gelu</span>(x):
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> x <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>tanh(np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>pi)<span style="color:#f92672">*</span>(x <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.044715</span><span style="color:#f92672">*</span>x<span style="color:#f92672">**</span><span style="color:#ae81ff">3</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> res
</span></span></code></pre></div>
  <time datetime="2024-03-05">2024-03-05&nbsp;</time>


  
    <div class="terminal-nav">
      <div class="back-nav">
        <a href="../" class="back-link">../</a>
      </div>
    </div>
  

  </main>
  <footer>
    <p>
  &copy; Copyright 2025 &middot;
  <a href="https://github.com/ntk148v/shibui">shibui</a>
</p>

  </footer>
</body>
</html>
